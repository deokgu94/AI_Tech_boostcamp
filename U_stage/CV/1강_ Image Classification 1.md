
OT
오태현 마스터님, 

컴퓨터 비젼 - 목차 : 
 - 컴퓨터비젼이 무엇인지?: 사진이나 비디오등을 통해 장면의 본질을 파악한다.
 
 준 근형대사 강의
 딥러닝 기반 기술만 : 패러다임 시프트 
 영어 강의 자료: .... 영어 공부로 열심히 해야겠다. 영어 표현에 익숙해지면 영어 논문을 읽기 편하기때문에 영어로 통일한다.
 
 ㄴ 남들이 정리하는 자료만 기다리면 뒤쳐지게 된다. 스스로 물고기를 낚이기 위해 영어로 논문을 읽는 계기가 되었으면 좋겠다.

<br>

## 1강: Image Calssification 1

### Computer vision이 무엇인지. 
AI란 "사람의 지능을 컴퓨터 시스템으로 구현"
 
 ㄴ 지능이란?: 인지능력, 기억, 이해 및 사고능력
 
   ㄴ 지능을 어떻게 시스템으로 구현하지? 
  
   -> 사람을 관찰(유아기) -> 세상과 교감하는 과정에서 보고, 듣고, 맛보고, 향즉 오감을 통해 상호작용이 된다. 

<br>   

앞으로 공부하게 공부하게 될것.
 - 기본적인 이미지 Taske
 - 단기간에 배우면서 실전에 효과적으로 쓸 수 있는 잔 테크닉(Augmentation and knowledge distillation)
 - 시각정보 중심의 다른 모델 데이터, 감각데이터의 융함
 - 생성모델의 컨트롤 가능한 버전인 Conditional generative model 
 - Visualization

<br>

### AlexNet 
이전과 다른점(LeNet-5)

 - 224 * 224 RGB images
 - 7 hidden layers
 - large amount of data
 - activation function-> ReLU
 - regularization technique (dropout)
 - 지금은 사용하는 않는 (Local Response Normalization(LRN))사용 ::: 대신해서 Batch normalization을 사용한다. 
 - 11x11 convolution filter를 사용했다.     ::: Receptive field의 size를 쉽게 크게 만들기 위해서 사용했으나 지금은 사용하지 않음
 
※ ReLu(Rectified Linear Unit): 입력값이 0보다 작으면 0, 0보다 크면 입력값 그대로 사용한다.
 ![image](https://user-images.githubusercontent.com/35412566/132293049-e75d9907-48be-4954-b33a-f573065e9c74.png)

 
※ Batch normalization: 배치 정규화는 평균과 분산을 조정하는 과정이 별도의 과정으로 떼어진 것이 아니라, 신경망 안에 포함되어 학습 시 평균과 분산을 조정하는 과정
  
  - 생성 배경: 히든 레이어들이 깊어짐에따라 Gradient의 변화량이 매우 작아지거나(Vanishing) 커진다면(Exploding) 효과적으로 학습시키지 못한다.
  
※ Receptive field: 외부 자극이 전체 영향을 끼치는 것이 아니라 특정 영역에만 영향을 준다는 뜻.
  - 현재는 주로 3x3을 사용하는데 히든 레이어들이 깊어짐에 따라 큰 필터싸이즈를 쓴것과 동일하게 작용한다고 한다. 
  
![image](https://user-images.githubusercontent.com/35412566/132270989-b4cbad50-47d6-4165-907e-57f1df07f5df.png)

※ 벡터화 

<br>

### VGGNet
이전과 다른점(AlextNet)
 - LRN을 사용하지 않았다.
 - 3x3 convolution filter를 사용했다. 
 - 2x2 max pooling operations 사용 
 - 3개의 fully-connected layers를 사용했다.  

※ max pooling operations: 합성곱 계층의 과적합(Overfitting)을 막기 위해, 출력 데이터의 크기를 줄이거나 특정 데이터를 강조하는 용도

<br>
<br>

## 2강: Annatation Ddata Efficient Learning
읽고 싶은 논문: 
 - Self-Transfer Learning for Fully Weakly Supervised Object Localization [@https://arxiv.org/abs/1602.01625]
 - The Simple and Efficient Semi-Supervised Learning Method for Deep Neural Networks [@http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=6EEAB47A6C95037AA28E470C740C16A3?doi=10.1.1.664.3543&rep=rep1&type=pdf]
 - RandAugment: Practical automated data augmentation with a reduced search space [@https://arxiv.org/abs/1909.13719]

3강: Image calssification 2
읽고 싶은 논문:
 - EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks [@https://arxiv.org/pdf/1905.11946.pdf]

